{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3f5aa7",
   "metadata": {},
   "source": [
    "**Put and run this notebook in the directory which contains TorchSpatial, because TorchSpatial will be used as a package. Relative imports are used within the package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "400ace9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "15013057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TorchSpatial.modules.trainer as trainer\n",
    "from TorchSpatial.modules.encoder_selector import get_loc_encoder\n",
    "from TorchSpatial.modules.model import ThreeLayerMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "c50dc1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bolongtang/Downloads/TorchSpatial/modules/trainer.py\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "print(trainer.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ae1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'TorchSpatial.modules.trainer' from '/Users/bolongtang/Downloads/TorchSpatial/modules/trainer.py'>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(trainer) # For easy reloading bypassing cache in case trainer.py gets edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "num_classes = 500 # birdsnap class count\n",
    "img_dim = loc_dim = embed_dim = 784 # Assumed, can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "8dca1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_dim = 2\n",
    "# - fake dataset: each row = (img_emb[784], latlon[2], class_index)\n",
    "N = 2048\n",
    "img = torch.randn(N, img_dim)                          # [N,784]\n",
    "\n",
    "lat = torch.rand(N)*180 - 90 # - 90 to 90\n",
    "lon = torch.rand(N)*360 # 0 to 360\n",
    "loc = torch.stack([lat, lon], dim=1) \n",
    "\n",
    "y = torch.randint(0, num_classes, (N,), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "971cb633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 784]) torch.Size([2048, 2]) torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape, loc.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b0875d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ximg_tr, Ximg_te, Xloc_tr, Xloc_te, y_tr, y_te = train_test_split(\n",
    "img, loc, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "0fea3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(Ximg_tr, Xloc_tr, y_tr))\n",
    "test_data  = list(zip(Ximg_te, Xloc_te, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Dataloader (loads image embeddings)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "train_loader2 = DataLoader(train_data, batch_size=32, shuffle=True) # For demonstration only; used below to not iterate through the actual train_loader, which is an iterator and each row only be accessed once per refill\n",
    "test_loader  = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ed806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) <class 'torch.Tensor'> tensor([[-2.7539,  0.3787,  1.5690,  ..., -2.0325, -3.0357, -0.7114],\n",
      "        [-1.2814, -1.3806,  0.2291,  ..., -0.6111, -0.2695, -0.5909],\n",
      "        [-1.4826, -1.0383, -1.0919,  ...,  1.4022,  0.5440,  0.7114],\n",
      "        ...,\n",
      "        [ 0.5787,  1.5012, -0.6779,  ...,  0.7032, -1.2775, -0.9090],\n",
      "        [-0.5163, -0.2988,  0.1979,  ...,  0.9552,  1.4006,  2.7824],\n",
      "        [ 0.9412,  1.0194, -0.7546,  ...,  0.0653, -0.0789, -0.2332]])\n",
      "torch.Size([32, 2]) <class 'torch.Tensor'> tensor([[-73.8534,  81.5765],\n",
      "        [-32.5084, 330.2889],\n",
      "        [-61.6616, 151.3372],\n",
      "        [-84.5772, 205.7255],\n",
      "        [-89.0486, 320.8131],\n",
      "        [ 16.0544, 340.5121],\n",
      "        [ 33.8640, 130.5281],\n",
      "        [ 49.6667, 351.4696],\n",
      "        [-20.7889,  63.1285],\n",
      "        [-39.4998, 224.6970],\n",
      "        [ 68.6975, 234.0212],\n",
      "        [ 15.4468, 298.8843],\n",
      "        [ 30.1908,  52.1262],\n",
      "        [ 82.4278, 256.9445],\n",
      "        [  3.9641, 332.7629],\n",
      "        [-57.6065, 103.2340],\n",
      "        [ 23.4334, 217.0275],\n",
      "        [-83.5055, 333.1920],\n",
      "        [-35.9738, 178.9625],\n",
      "        [-41.2283,  45.7941],\n",
      "        [ 29.4503, 210.9034],\n",
      "        [-28.8177, 246.1757],\n",
      "        [-50.1442,  76.8163],\n",
      "        [-40.4104, 187.1286],\n",
      "        [-82.2237, 290.7935],\n",
      "        [-70.0817, 316.8458],\n",
      "        [  3.4689, 132.5819],\n",
      "        [ 51.8204, 129.9941],\n",
      "        [-44.8214, 285.9138],\n",
      "        [-37.7025,  18.5469],\n",
      "        [-54.4267, 232.1530],\n",
      "        [-88.7174, 108.7690]])\n",
      "torch.Size([32]) <class 'torch.Tensor'> tensor([455, 455, 425, 150, 397, 141,  78, 312,   6,  51, 362,  55, 157, 161,\n",
      "        162, 324, 440,  32, 296, 123,  50, 247, 428, 408, 211, 308, 102, 330,\n",
      "        343, 259, 465, 319])\n"
     ]
    }
   ],
   "source": [
    "for img_b, loc_b, y_b in train_loader2:\n",
    "    print(img_b.shape, type(img_b), img_b) # Should be random floats\n",
    "    print(loc_b.shape, type(loc_b), loc_b) # lat [-90, 90] and lon [0, 360]\n",
    "    print(y_b.shape, type(y_b), y_b) # random ints [0, 500]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - location encoder\n",
    "# Allowed: Space2Vec-grid, Space2Vec-theory, xyz, NeRF, Sphere2Vec-sphereC, Sphere2Vec-sphereC+, Sphere2Vec-sphereM, Sphere2Vec-sphereM+, Sphere2Vec-dfs, rbf, rff, wrap, wrap_ffn, tile_ffn\n",
    "# overrides is a dictionary that allows overriding specific params. \n",
    "# ex. loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"max_radius\":7800, \"min_radius\":15, \"spa_embed_dim\":784, \"device\": device})\n",
    "loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"coord_dim\": coord_dim, \"spa_embed_dim\": loc_dim, \"device\": device}) # \"device\": device is needed to prevent AssertionError: Torch not compiled with CUDA enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "2868bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model\n",
    "# decoder = ThreeLayerMLP(input_dim = embed_dim, hidden_dim = 1024, category_count = num_classes)\n",
    "model = loc_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "5f552086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# - Optimizer\n",
    "optimizer = Adam(params = model.parameters(), lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "b7fd5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, batch    15] loss: 6.665\n",
      "[epoch 1, batch    30] loss: 6.664\n",
      "[epoch 1, batch    45] loss: 6.664\n",
      "[epoch 2, batch    15] loss: 6.664\n",
      "[epoch 2, batch    30] loss: 6.665\n",
      "[epoch 2, batch    45] loss: 6.665\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "# - train() \n",
    "trainer.train(epochs = 2, \n",
    "        batch_count_print_avg_loss = 15,\n",
    "        loc_encoder = loc_encoder,\n",
    "        dataloader = train_loader,\n",
    "        model = model,\n",
    "        criterion = criterion,\n",
    "        optimizer = optimizer,\n",
    "        device = device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "d618589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 410 test images: 0.24%\n"
     ]
    }
   ],
   "source": [
    "# - test\n",
    "model.eval()\n",
    "total = correct = 0\n",
    "with torch.no_grad():\n",
    "    for img_b, loc_b, y_b in test_loader:\n",
    "        img_b, loc_b, y_b = img_b.to(device), loc_b.to(device), y_b.to(device)\n",
    "        img_embedding = img_b\n",
    "        loc_embedding = trainer.forward_with_np_array(batch_data = loc_b, model = model)\n",
    "\n",
    "        loc_img_interaction_embedding = torch.mul(loc_embedding, img_embedding)\n",
    "        outputs = torch.nn.Softmax(dim=1)(loc_img_interaction_embedding)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        total += y_b.size(0)\n",
    "        correct += (pred == y_b).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
