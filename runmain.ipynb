{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3f5aa7",
   "metadata": {},
   "source": [
    "**Put and run this notebook in the directory which contains TorchSpatial, because TorchSpatial will be used as a package. Relative imports are used within the package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ace9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "15013057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TorchSpatial.modules.trainer as trainer\n",
    "from TorchSpatial.modules.encoder_selector import get_loc_encoder\n",
    "import TorchSpatial.modules.model as premade_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c50dc1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bolongtang/Downloads/TorchSpatial/modules/trainer.py\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "print(trainer.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "669ae1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'TorchSpatial.modules.model' from '/Users/bolongtang/Downloads/TorchSpatial/modules/model.py'>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For easy reloading, bypassing cache in case trainer.py gets edited\n",
    "importlib.reload(trainer) \n",
    "\n",
    "importlib.reload(premade_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1a55c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "num_classes = 500 # birdsnap class count\n",
    "img_dim = loc_dim = embed_dim = 784 # Assumed, can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8dca1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_dim = 2\n",
    "# - fake dataset: each row = (img_emb[784], latlon[2], class_index)\n",
    "N = 2048\n",
    "img = torch.randn(N, img_dim) * 10                         # [N,784]\n",
    "\n",
    "lat = torch.rand(N)*180 - 90 # - 90 to 90\n",
    "lon = torch.rand(N)*360 # 0 to 360\n",
    "loc = torch.stack([lat, lon], dim=1) \n",
    "\n",
    "y = torch.randint(0, num_classes, (N,), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "412d16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured relationship: class depends on latitude and longitude bands\n",
    "num_lat_bands = 10\n",
    "num_lon_bands = 10\n",
    "lat_band = ((lat + 90) // (180 / num_lat_bands)).long()\n",
    "lon_band = (lon // (360 / num_lon_bands)).long()\n",
    "y = (lat_band * num_lon_bands + lon_band) % num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "971cb633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 784]) torch.Size([2048, 2]) torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape, loc.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b0875d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ximg_tr, Ximg_te, Xloc_tr, Xloc_te, y_tr, y_te = train_test_split(\n",
    "img, loc, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0fea3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(Ximg_tr, Xloc_tr, y_tr))\n",
    "test_data  = list(zip(Ximg_te, Xloc_te, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "398a5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Dataloader (loads image embeddings)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "train_loader2 = DataLoader(train_data, batch_size=32, shuffle=True) # For demonstration only; used below to not iterate through the actual train_loader, which is an iterator and each row only be accessed once per refill\n",
    "test_loader  = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ce1ed806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) <class 'torch.Tensor'> tensor([[ -1.3605, -16.6912,  -5.3204,  ...,  -5.2019,  -7.5185,  -5.2076],\n",
      "        [  9.1958,  10.0931,   0.6847,  ...,  -8.6666,  14.4780,  14.1578],\n",
      "        [ -3.4650,  -7.0869,   1.6418,  ...,  -1.9308, -10.9980, -13.4757],\n",
      "        ...,\n",
      "        [  7.8177,   2.1599,   8.6928,  ..., -15.9909,   2.0767, -17.8155],\n",
      "        [ -5.1476,   6.0597,  -9.5395,  ..., -28.3844,  -7.6159,  -1.2148],\n",
      "        [  3.1484,  -8.5589,  16.1132,  ..., -18.9965,   4.7044,  -3.6413]])\n",
      "torch.Size([32, 2]) <class 'torch.Tensor'> tensor([[ 26.2840, 101.5449],\n",
      "        [-58.9376, 274.1528],\n",
      "        [-12.5994,  46.0138],\n",
      "        [-50.0492,   7.4704],\n",
      "        [ -4.6779, 163.9617],\n",
      "        [  4.8002, 207.1658],\n",
      "        [-37.9944,   0.7866],\n",
      "        [-49.5741,  67.7224],\n",
      "        [-57.6029, 200.6449],\n",
      "        [-63.0124, 128.2206],\n",
      "        [-86.2934, 245.2606],\n",
      "        [-22.7024,  47.9350],\n",
      "        [-63.4830, 304.7218],\n",
      "        [-32.0722, 263.2834],\n",
      "        [  6.6238, 263.9005],\n",
      "        [ 11.3696,  71.8548],\n",
      "        [ 80.7367,  77.6899],\n",
      "        [ 50.6377, 315.3993],\n",
      "        [-56.4612,  17.3814],\n",
      "        [-18.5066,  85.9361],\n",
      "        [-73.6218,  77.5226],\n",
      "        [-25.6574,   8.2528],\n",
      "        [-33.8498, 109.2964],\n",
      "        [ 27.6664, 299.8725],\n",
      "        [ 15.0578,   6.2198],\n",
      "        [ 50.6540, 330.8817],\n",
      "        [ 53.5352, 280.4719],\n",
      "        [ 82.5920, 261.3139],\n",
      "        [-43.6406, 149.3593],\n",
      "        [ 21.4727, 151.3234],\n",
      "        [-43.1812, 102.9398],\n",
      "        [-52.1906, 141.9287]])\n",
      "torch.Size([32]) <class 'torch.Tensor'> tensor([62, 17, 41, 20, 44, 55, 20, 21, 15, 13,  6, 31, 18, 37, 57, 51, 92, 78,\n",
      "        10, 32,  2, 30, 33, 68, 50, 79, 77, 97, 24, 64, 22, 23])\n"
     ]
    }
   ],
   "source": [
    "for img_b, loc_b, y_b in train_loader2:\n",
    "    print(img_b.shape, type(img_b), img_b) # Should be random floats\n",
    "    print(loc_b.shape, type(loc_b), loc_b) # lat [-90, 90] and lon [0, 360]\n",
    "    print(y_b.shape, type(y_b), y_b) # random ints [0, 500]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - location encoder\n",
    "# Allowed: Space2Vec-grid, Space2Vec-theory, xyz, NeRF, Sphere2Vec-sphereC, Sphere2Vec-sphereC+, Sphere2Vec-sphereM, Sphere2Vec-sphereM+, Sphere2Vec-dfs, rbf, rff, wrap, wrap_ffn, tile_ffn\n",
    "# overrides is a dictionary that allows overriding specific params. \n",
    "# ex. loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"max_radius\":7800, \"min_radius\":15, \"spa_embed_dim\":784, \"device\": device})\n",
    "loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"coord_dim\": coord_dim, \"spa_embed_dim\": loc_dim, \"device\": device}) # \"device\": device is needed if you defined device = 'cpu' above and don't have cuda setup to prevent \"AssertionError: Torch not compiled with CUDA enabled\", because the default is device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2868bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model\n",
    "decoder = premade_models.ThreeLayerMLP(input_dim = embed_dim, hidden_dim = 1024, category_count = num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5f552086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# - Optimizer\n",
    "optimizer = Adam(params = list(loc_encoder.ffn.parameters()) + list(decoder.parameters()), lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b7fd5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, batch    15] loss: 5.731\n",
      "[epoch 1, batch    30] loss: 4.951\n",
      "[epoch 1, batch    45] loss: 4.811\n",
      "[epoch 2, batch    15] loss: 4.612\n",
      "[epoch 2, batch    30] loss: 4.594\n",
      "[epoch 2, batch    45] loss: 4.582\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "# - train() \n",
    "trainer.train(epochs = 2, \n",
    "        batch_count_print_avg_loss = 15,\n",
    "        loc_encoder = loc_encoder,\n",
    "        dataloader = train_loader,\n",
    "        decoder = decoder,\n",
    "        criterion = criterion,\n",
    "        optimizer = optimizer,\n",
    "        device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d618589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 410 test images: 1.46%\n"
     ]
    }
   ],
   "source": [
    "# - test\n",
    "decoder.eval()\n",
    "total = correct = 0\n",
    "with torch.no_grad():\n",
    "    for img_b, loc_b, y_b in test_loader:\n",
    "        img_b, loc_b, y_b = img_b.to(device), loc_b.to(device), y_b.to(device)\n",
    "        img_embedding = img_b\n",
    "        loc_embedding = trainer.forward_with_np_array(batch_data = loc_b, model = model)\n",
    "\n",
    "        loc_img_interaction_embedding = torch.mul(loc_embedding, img_embedding)\n",
    "        outputs = torch.nn.Softmax(dim=1)(loc_img_interaction_embedding)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        total += y_b.size(0)\n",
    "        correct += (pred == y_b).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
