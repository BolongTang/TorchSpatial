{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3f5aa7",
   "metadata": {},
   "source": [
    "**Put and run this notebook in the directory which contains TorchSpatial, because TorchSpatial will be used as a package. Relative imports are used within the package**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433dc4d",
   "metadata": {},
   "source": [
    "**Model Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400ace9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15013057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TorchSpatial.modules.trainer as trainer\n",
    "from TorchSpatial.modules.encoder_selector import get_loc_encoder\n",
    "import TorchSpatial.modules.model as premade_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50dc1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bolongtang/Downloads/TorchSpatial/modules/trainer.py\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "print(trainer.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669ae1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'TorchSpatial.modules.model' from '/Users/bolongtang/Downloads/TorchSpatial/modules/model.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For easy reloading, bypassing cache in case trainer.py gets edited\n",
    "importlib.reload(trainer) \n",
    "\n",
    "importlib.reload(premade_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a55c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "num_classes = 500 # birdsnap class count\n",
    "img_dim = loc_dim = embed_dim = 784 # Assumed, can change.\n",
    "coord_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979d3fa",
   "metadata": {},
   "source": [
    "**Random Data With Patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dca1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - fake dataset: each row = (img_emb[784], latlon[2], class_index)\n",
    "N = 2048\n",
    "img = torch.randn(N, img_dim) * 10                         # [N,784]\n",
    "\n",
    "lat = torch.rand(N)*180 - 90 # - 90 to 90\n",
    "lon = torch.rand(N)*360 # 0 to 360\n",
    "loc = torch.stack([lat, lon], dim=1) \n",
    "\n",
    "y = torch.randint(0, num_classes, (N,), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412d16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured relationship: class depends on latitude and longitude bands\n",
    "num_lat_bands = 10\n",
    "num_lon_bands = 10\n",
    "lat_band = ((lat + 90) // (180 / num_lat_bands)).long()\n",
    "lon_band = (lon // (360 / num_lon_bands)).long()\n",
    "y = (lat_band * num_lon_bands + lon_band) % num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "971cb633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 784]) torch.Size([2048, 2]) torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape, loc.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0875d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ximg_tr, Ximg_te, Xloc_tr, Xloc_te, y_tr, y_te = train_test_split(\n",
    "img, loc, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fea3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(Ximg_tr, Xloc_tr, y_tr))\n",
    "test_data  = list(zip(Ximg_te, Xloc_te, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398a5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Dataloader (loads image embeddings)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "train_loader2 = DataLoader(train_data, batch_size=32, shuffle=True) # For demonstration only; used below to not iterate through the actual train_loader, which is an iterator and each row only be accessed once per refill\n",
    "test_loader  = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce1ed806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) <class 'torch.Tensor'> tensor([[ -9.9439,  18.0293,  10.1675,  ...,   0.2642, -10.9086,  -1.5463],\n",
      "        [ 10.6064,  -0.1239,   4.8373,  ...,  -3.8627,   1.7590,   8.2846],\n",
      "        [ -6.2455,   8.5587,   0.7948,  ...,  -7.0476,  -1.3425,  -7.0909],\n",
      "        ...,\n",
      "        [-14.2805,  11.9501,   7.2585,  ..., -11.6572,   5.7310,  -2.8456],\n",
      "        [ -2.3232,  -3.4536,  15.2739,  ...,  -9.6967,  -6.8765,  -1.6661],\n",
      "        [ 23.0095,  -6.4897,  10.7217,  ...,  10.3794, -13.2433,  -1.1273]])\n",
      "torch.Size([32, 2]) <class 'torch.Tensor'> tensor([[-48.8777, 261.4194],\n",
      "        [ 54.4361, 348.2578],\n",
      "        [ 82.2791, 282.2256],\n",
      "        [-84.3648, 115.8486],\n",
      "        [-34.4906, 222.1841],\n",
      "        [  6.9828, 116.2994],\n",
      "        [ -7.0862, 169.4892],\n",
      "        [ 43.2347,  39.8236],\n",
      "        [-10.1921,  32.9706],\n",
      "        [ 14.0709, 115.6781],\n",
      "        [ 30.7118, 156.0988],\n",
      "        [-21.1795, 130.9596],\n",
      "        [ 41.5294,   0.5229],\n",
      "        [-61.1150, 219.3945],\n",
      "        [ 59.7967, 210.3055],\n",
      "        [-36.4783, 342.9156],\n",
      "        [-10.0109, 288.2186],\n",
      "        [-85.0008,  71.8624],\n",
      "        [-13.0062, 352.8879],\n",
      "        [-21.1816, 114.2190],\n",
      "        [-84.2352, 138.9985],\n",
      "        [ 79.2919, 167.6269],\n",
      "        [ 11.5537, 256.9916],\n",
      "        [ 43.0308, 179.9374],\n",
      "        [ 74.0693,  42.6047],\n",
      "        [-34.2444, 183.3537],\n",
      "        [ 33.6151, 153.1748],\n",
      "        [ 16.2512,  77.0168],\n",
      "        [ 18.5193, 292.1176],\n",
      "        [ 64.6232, 192.7084],\n",
      "        [-77.5204, 322.8724],\n",
      "        [ 72.8904,  36.0300]])\n",
      "torch.Size([32]) <class 'torch.Tensor'> tensor([27, 89, 97,  3, 36, 53, 44, 71, 40, 53, 64, 33, 70, 16, 85, 29, 48,  1,\n",
      "        49, 33,  3, 94, 57, 74, 91, 35, 64, 52, 68, 85,  8, 91])\n"
     ]
    }
   ],
   "source": [
    "for img_b, loc_b, y_b in train_loader2:\n",
    "    print(img_b.shape, type(img_b), img_b) # Should be random floats\n",
    "    print(loc_b.shape, type(loc_b), loc_b) # lat [-90, 90] and lon [0, 360]\n",
    "    print(y_b.shape, type(y_b), y_b) # random ints [0, 500]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f19a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - location encoder\n",
    "# Allowed: Space2Vec-grid, Space2Vec-theory, xyz, NeRF, Sphere2Vec-sphereC, Sphere2Vec-sphereC+, Sphere2Vec-sphereM, Sphere2Vec-sphereM+, Sphere2Vec-dfs, rbf, rff, wrap, wrap_ffn, tile_ffn\n",
    "# overrides is a dictionary that allows overriding specific params. \n",
    "# ex. loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"max_radius\":7800, \"min_radius\":15, \"spa_embed_dim\":784, \"device\": device})\n",
    "loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"coord_dim\": coord_dim, \"spa_embed_dim\": loc_dim, \"device\": device}) # \"device\": device is needed if you defined device = 'cpu' above and don't have cuda setup to prevent \"AssertionError: Torch not compiled with CUDA enabled\", because the default is device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2868bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model\n",
    "decoder = premade_models.ThreeLayerMLP(input_dim = embed_dim, hidden_dim = 1024, category_count = num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f552086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# - Optimizer\n",
    "optimizer = Adam(params = list(loc_encoder.ffn.parameters()) + list(decoder.parameters()), lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7fd5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, batch    30] loss: 5.290\n",
      "[epoch 2, batch    30] loss: 4.560\n",
      "[epoch 3, batch    30] loss: 4.474\n",
      "[epoch 4, batch    30] loss: 4.276\n",
      "[epoch 5, batch    30] loss: 4.035\n",
      "[epoch 6, batch    30] loss: 3.652\n",
      "[epoch 7, batch    30] loss: 3.340\n",
      "[epoch 8, batch    30] loss: 2.854\n",
      "[epoch 9, batch    30] loss: 2.567\n",
      "[epoch 10, batch    30] loss: 2.375\n",
      "[epoch 11, batch    30] loss: 2.112\n",
      "[epoch 12, batch    30] loss: 1.883\n",
      "[epoch 13, batch    30] loss: 1.639\n",
      "[epoch 14, batch    30] loss: 1.552\n",
      "[epoch 15, batch    30] loss: 1.433\n",
      "[epoch 16, batch    30] loss: 1.283\n",
      "[epoch 17, batch    30] loss: 1.224\n",
      "[epoch 18, batch    30] loss: 1.189\n",
      "[epoch 19, batch    30] loss: 1.020\n",
      "[epoch 20, batch    30] loss: 1.141\n",
      "[epoch 21, batch    30] loss: 0.920\n",
      "[epoch 22, batch    30] loss: 1.014\n",
      "[epoch 23, batch    30] loss: 0.930\n",
      "[epoch 24, batch    30] loss: 0.959\n",
      "[epoch 25, batch    30] loss: 0.978\n",
      "[epoch 26, batch    30] loss: 0.815\n",
      "[epoch 27, batch    30] loss: 0.869\n",
      "[epoch 28, batch    30] loss: 0.896\n",
      "[epoch 29, batch    30] loss: 0.871\n",
      "[epoch 30, batch    30] loss: 0.879\n",
      "[epoch 31, batch    30] loss: 0.883\n",
      "[epoch 32, batch    30] loss: 0.835\n",
      "[epoch 33, batch    30] loss: 0.800\n",
      "[epoch 34, batch    30] loss: 0.656\n",
      "[epoch 35, batch    30] loss: 0.700\n",
      "[epoch 36, batch    30] loss: 0.768\n",
      "[epoch 37, batch    30] loss: 0.686\n",
      "[epoch 38, batch    30] loss: 0.915\n",
      "[epoch 39, batch    30] loss: 0.776\n",
      "[epoch 40, batch    30] loss: 0.713\n",
      "[epoch 41, batch    30] loss: 0.691\n",
      "[epoch 42, batch    30] loss: 0.576\n",
      "[epoch 43, batch    30] loss: 0.606\n",
      "[epoch 44, batch    30] loss: 0.679\n",
      "[epoch 45, batch    30] loss: 0.583\n",
      "[epoch 46, batch    30] loss: 0.696\n",
      "[epoch 47, batch    30] loss: 0.556\n",
      "[epoch 48, batch    30] loss: 0.732\n",
      "[epoch 49, batch    30] loss: 0.603\n",
      "[epoch 50, batch    30] loss: 0.697\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "# - train() \n",
    "epochs = 50\n",
    "trainer.train(epochs = epochs, \n",
    "        batch_count_print_avg_loss = 30,\n",
    "        loc_encoder = loc_encoder,\n",
    "        dataloader = train_loader,\n",
    "        decoder = decoder,\n",
    "        criterion = criterion,\n",
    "        optimizer = optimizer,\n",
    "        device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d618589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy on 410 test images: 6.10%\n",
      "Top-3 Accuracy on 410 test images: 15.37%\n",
      "MRR on 410 test images: 0.1521\n"
     ]
    }
   ],
   "source": [
    "# - test\n",
    "loc_encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "total = 0\n",
    "correct_top1 = 0\n",
    "correct_top3 = 0\n",
    "mrr_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_b, loc_b, y_b in test_loader:\n",
    "        img_b, loc_b, y_b = img_b.to(device), loc_b.to(device), y_b.to(device)\n",
    "\n",
    "        img_embedding = img_b\n",
    "        loc_embedding = trainer.forward_with_np_array(batch_data=loc_b, model=loc_encoder)\n",
    "\n",
    "        loc_img_interaction_embedding = torch.mul(loc_embedding, img_embedding)\n",
    "        logits = decoder(loc_img_interaction_embedding)\n",
    "\n",
    "        # Top-1\n",
    "        pred = logits.argmax(dim=1)\n",
    "\n",
    "        # Top-3 accuracy\n",
    "        top3_idx = logits.topk(3, dim=1).indices                    # [B, 3]\n",
    "        correct_top3 += (top3_idx == y_b.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "        # MRR (full ranking over all classes)\n",
    "        ranking = logits.argsort(dim=1, descending=True)             # [B, C]\n",
    "        positions = ranking.argsort(dim=1)                           # [B, C] where positions[b, c] = rank index (0-based)\n",
    "        true_pos0 = positions.gather(1, y_b.view(-1, 1)).squeeze(1)  # [B]\n",
    "        mrr_sum += (1.0 / (true_pos0.float() + 1.0)).sum().item()\n",
    "\n",
    "        total += y_b.size(0)\n",
    "        correct_top1 += (pred == y_b).sum().item()\n",
    "\n",
    "top1_acc = 100.0 * correct_top1 / total if total else 0.0\n",
    "top3_acc = 100.0 * correct_top3 / total if total else 0.0\n",
    "mrr = mrr_sum / total if total else 0.0\n",
    "\n",
    "print(f\"Top-1 Accuracy on {total} test images: {top1_acc:.2f}%\")\n",
    "print(f\"Top-3 Accuracy on {total} test images: {top3_acc:.2f}%\")\n",
    "print(f\"MRR on {total} test images: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd204f9",
   "metadata": {},
   "source": [
    "**Model saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "005c3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def save_model(loc_encoder, decoder, optimizer, epoch, path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"loc_encoder\": loc_encoder.state_dict(),\n",
    "        \"decoder\": decoder.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c03132",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\n",
    "    loc_encoder=loc_encoder,\n",
    "    decoder=decoder,\n",
    "    optimizer=optimizer,\n",
    "    epoch=epochs,\n",
    "    path=\"TorchSpatial/checkpoints/final.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c917e",
   "metadata": {},
   "source": [
    "**Use Saved Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc7b5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_encoder = get_loc_encoder(name = \"Space2Vec-grid\", overrides = {\"coord_dim\": coord_dim, \"spa_embed_dim\": loc_dim, \"device\": device}) # \"device\": device is needed if you defined device = 'cpu' above and don't have cuda setup to prevent \"AssertionError: Torch not compiled with CUDA enabled\", because the default is device=\"cuda\"\n",
    "decoder = premade_models.ThreeLayerMLP(input_dim = embed_dim, hidden_dim = 1024, category_count = num_classes).to(device)\n",
    "optimizer = Adam(params = list(loc_encoder.ffn.parameters()) + list(decoder.parameters()), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138d31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"TorchSpatial/checkpoints/final.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f185eda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_encoder.load_state_dict(ckpt[\"loc_encoder\"])\n",
    "decoder.load_state_dict(ckpt[\"decoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f79c7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "start_epoch = ckpt[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e30dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy on 410 test images: 6.10%\n",
      "Top-3 Accuracy on 410 test images: 15.37%\n",
      "MRR on 410 test images: 0.1521\n"
     ]
    }
   ],
   "source": [
    "# - test\n",
    "loc_encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "total = 0\n",
    "correct_top1 = 0\n",
    "correct_top3 = 0\n",
    "mrr_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_b, loc_b, y_b in test_loader:\n",
    "        img_b, loc_b, y_b = img_b.to(device), loc_b.to(device), y_b.to(device)\n",
    "\n",
    "        img_embedding = img_b\n",
    "        loc_embedding = trainer.forward_with_np_array(batch_data=loc_b, model=loc_encoder)\n",
    "\n",
    "        loc_img_interaction_embedding = torch.mul(loc_embedding, img_embedding)\n",
    "        logits = decoder(loc_img_interaction_embedding)\n",
    "\n",
    "        # Top-1\n",
    "        pred = logits.argmax(dim=1)\n",
    "\n",
    "        # Top-3 accuracy\n",
    "        top3_idx = logits.topk(3, dim=1).indices                    # [B, 3]\n",
    "        correct_top3 += (top3_idx == y_b.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "        # MRR (full ranking over all classes)\n",
    "        ranking = logits.argsort(dim=1, descending=True)             # [B, C]\n",
    "        positions = ranking.argsort(dim=1)                           # [B, C] where positions[b, c] = rank index (0-based)\n",
    "        true_pos0 = positions.gather(1, y_b.view(-1, 1)).squeeze(1)  # [B]\n",
    "        mrr_sum += (1.0 / (true_pos0.float() + 1.0)).sum().item()\n",
    "\n",
    "        total += y_b.size(0)\n",
    "        correct_top1 += (pred == y_b).sum().item()\n",
    "\n",
    "top1_acc = 100.0 * correct_top1 / total if total else 0.0\n",
    "top3_acc = 100.0 * correct_top3 / total if total else 0.0\n",
    "mrr = mrr_sum / total if total else 0.0\n",
    "\n",
    "print(f\"Top-1 Accuracy on {total} test images: {top1_acc:.2f}%\")\n",
    "print(f\"Top-3 Accuracy on {total} test images: {top3_acc:.2f}%\")\n",
    "print(f\"MRR on {total} test images: {mrr:.4f}\")\n",
    "\n",
    "# Results below match the final model pre-saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
